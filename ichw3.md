#  高速缓存存储器（Cache）结构及工作原理

##  一、Cache是什么？

　　Cache，高速缓存存储器，是计算机中介于寄存器与主存储器之间的器件，在工作中频繁的接受CPU的访问并读写Cache上的信息。

## 二、Cache有什么用？

![image](https://raw.githubusercontent.com/Dead-fisher/ichw/master/TIM%E5%9B%BE%E7%89%8720181003172429.png)

　　众所周知，CPU是电脑中最核心最金贵的部件，许多大难度计算都由CPU完成，因此CPU更新换代的速度极快，计算速度在不断的提高。但是与之相背的是，主存储器的读取数据的速度却无法与之相适应，这导致出现CPU处理速度过快以至于等待读取数据的现象，这使CPU的效率大打折扣。
  
  　为了解决这一问题，Cache横空出世，它被用于CPU与主存储器之间，提前读取主存储器的数据放入自己的储存空间中，等待CPU的访问；同时，Cache与CPU的读取速率比主存储器快的多，因此大大避免了CPU的等待情况的出现。

　　相比于主存储器来说，Cache的存储空间更小，读取速度较高，起到了连接主存储器和CPU的桥梁作用。由于Cache的存取速率相当快，使得CPU的利用率大大提高，进而使整个系统的性能得以提升。

## 三、Cache的结构是什么样子的？

![image](https://raw.githubusercontent.com/Dead-fisher/ichw/master/20171115142152426.png)

该结构简化在平面中为：

![image](https://raw.githubusercontent.com/Dead-fisher/ichw/master/TIM%E5%9B%BE%E7%89%8720181003203127.png)

　　此两图为Cache的结构简图。

（其中，Data为储存的数据，Tag为数据的地址信息，Valid则指出了该数据是否保留了有效位数，即能否有效被调用。）

![image](https://raw.githubusercontent.com/Dead-fisher/ichw/master/Inked%E4%B8%BB%E5%AD%98%E4%B8%8E%E7%BC%93%E5%AD%98_LI.jpg)

　　事实上，主存和Cache的结构基本相同，都含有数据和地址，不过不同的是，在Cache中，多出了地址信息一栏，用于存储该数据对应物理内存的地址。当CPU访问Cache时，同时可以得到该数据在主存中的地址，从而得到物理内存中相应的数据。有效位的引入则是为了方便CPU更快的找到有效数据。

　　在主存和缓存中，数据都是以块（block）为单位进行存储的，假设主存有M块，缓存有C块，那么就有M>>C，也就是说，主存的存储量要远远大于缓存的存储量。


　　而对于被从主存中提取并放入缓存的数据来说，其在主存的数据与在缓存的数据完全相同，因此只要地址信息准确，CPU可以直接从Cache中进行读取到正确的数据并进行读写，但由于主存和缓存的块（block）的大小不同，因此如果数据过大，可能无法完整的放入缓存内，此时便需要CPU访问主存。
  
  ## 四、Cache是如何工作的？

　　**要了解这个问题，我们首先需要明确几个概念：**
  
  ### 1. 局部性原理与二八法则
  
 　　**时间局部性**（temporal locality） ：
 
时间局部性指的是：被引用过一次的存储器位置在未来会被多次引用（通常在循环中）。

　　**空间局部性**（spatial locality） ：
  
  
如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。
  
　　此外还有**程序的局部性原理**：
  
  也就是我们常说的二八法则，一段程序中执行频率最高的代码往往只有20%，80%的代码很少用到，而这20％的代码完成了整个程序80%的功能。
  
  
  ### 2. 命中与未命中
  
 　　刚才讲到，由于缓存的存储空间远小于内存的存储空间，势必无法储存所有的数据。因此我们规定，当CPU访问缓存时，如果需要的数据在Cache内，则成为**命中**；若不在Cache内，则称为**未命中**。
   
　　同时，定义**命中率**：即一段时间内，命中的次数与该时间内CPU访问的总次数的比值。由此我们知道，Cache的命中率当然是越高越好。一般来说，Cache的容量越大，命中率越高。其次，命中率也与算法息息相关。
  
  ### 3. 缓存的数据与主存的数据一一对应，因此存在映射关系，其具体大致分为如下几种：
  
　　#### a.全相连映射
  
　　主存中任何一个块均可以映像装入到Cache中的任何一个块的位置上。主存地址分为块号和块内地址两部分，Cache地址也分为块号和块内地址。Cache的块内地址部分直接取自主存地址的块内地址段。主存块号和Cache块号不相同，Cache块号根据主存块号从块表中查找。Cache保存的各数据块互不相关，Cache必须对每个块和块自身的地址加以存储。当请求数据时，Cache控制器要把请求地址同所有的地址加以比较，进行确认。
   
　　主存地址格式：主存块号+块内偏移地址

　　cache地址格式： cache行号+行内偏移地址

　　cache标记tag：主存块号
  
　　映射过程（地址变换过程）：
  
　　CPU提供一内存地址给cache，cache中的“控制逻辑”将“主存地址格式”中的“主存块号”与cache中所有行的标记tag进行同时比较。
  
　　如果存在相同的，即表示“命中”，根据“块内偏移地址”找到相应的字。
  
　　如果不存在相同的，即表示“未命中”，那么将会到主存中寻找。
  
  ![image](https://raw.githubusercontent.com/Dead-fisher/pictures/master/%E5%85%A8.png)


　　#### b.直接映射
  
  把主存分成若干区，每区与Cache大小相同。区内分块，主存每个区中块的大小和Cache中块的大小相等，主存中每个区包含的块的个数与Cache中块的个数相等。任意一个主存块只能映像到Cache中唯一指定的块中，即相同块号的位置。主存地址分为三部分：区号、块号和块内地址，Cache地址分为：块号和块内地址。直接映像方式下，数据块只能映像到Cache中唯一指定的位置，故不存在替换算法的问题。它不同于全相连Cache，地址仅需比较一次。

　　主存地址格式：主存组号+组内块号+块内偏移地址

　　cache地址格式：cache行号+行内偏移地址

　　cache标记tag：映射到该行的主存块的主存地址的“组号”
  
　　映射过程（地址变换过程）：
  
　　CPU提供一内存地址给cache，相关的逻辑根据内存地址中的“组内块号”确定该主存块如果发生拷贝会被拷贝到哪一行；然后，将内存地址中的“主存组号”与上步确定的cache行的标记tag进行比较，如果存在相同的即“命中”，如果不存在相同的即“未命中”。
  
  ![image](https://raw.githubusercontent.com/Dead-fisher/pictures/master/%E7%9B%B4.png)
  
  
　　#### c.组相连映射
  
　　组相连映像是前两种方式的折衷。主存按Cache容量分区，每个区分为若干组，每组包含若干块。Cache也进行同样的分组和分块。主存中一个组内的块数与Cache中一个组内的块数相等。组间采用直接方式，组内采用全相连方式。组的容量＝1时，即直接映像，组的容量＝整个Cache的容量时，即全相连映像。Cache的存在对于程序员透明，Cache的地址变换和数据块的替换算法都采用硬件实现。
  
　　主存地址格式：主存组号+组内块号+块内偏移地址

　　cache地址格式：cache组号+组内行号+行内偏移地址

　　cache标记tag：组号
  
　　映射过程（地址变换过程）：
CPU提供一内存地址给cache，相关逻辑根据地址中的“组内块号”部分确定主存块如果发生拷贝将会被放置到cache的哪一组中；
然后，将地址中的“主存组号”与上步所确定的那一组中所有行的tag同时进行比较，如果存在相同的即“命中”，如果不存在相同的即为“未命中”。

![image](https://raw.githubusercontent.com/Dead-fisher/pictures/master/%E7%BB%84.png)




　　**了解了这些之后，接下来，我们来介绍Cache具体的工作过程**
  
  ![image](https://raw.githubusercontent.com/Dead-fisher/ichw/master/2RI7F2WYUB1ESWB%40M%40YE%7D8O.png)
  
  
　　简单来说，工作原理是把CPU最近可能用到的少量信息，可能是数据，也可能是指令，从内存复制到CACHE中，使CPU能够更高速的访问这些数据，提高工作效率。
  
　　首先CPU对Cache发出访问，通过主存Cache地址映射变换机制没来判断该数据是否命中（判断方法如上文所示）：
  
　　若**命中**，则直接从Cache读取；
     
　　若**未命中**，则需要从主存中调入数据到缓存中。
  
　　若需要调入数据则需要先判断，缓存中是否还存在空间能够提供给将要调入的数据：
  
　　如果**存在**空间，则直接调入即可；
  
　　如果**不存在**空间，则需要采取数据的替换策略，将缓存中的一部分数据调出以腾出空间，此时便用到替换算法。（某些Cache为了提高访问数据的速度，会利用数据总线，当Cache未命中时，CPU会通过主线同时从内存中调用数据）
  
　　当然，这些还没有完，在Cache的不断改进下，现在采用的大多是多级缓存。即采用多个Cache同时工作的方法，在CPU和主存之间设立多个Cache。效率最高的Cache直接设立在CPU内部，其次在CPU外部以及主存外部再各设立一个Cache，因此现在的计算机大多具有三个以上的Cache。当然，如果是多核处理器（CPU），那么每一个CPU都会有自己独立的Cache，以提高工作效率（但同时miss率会上升），以及存在多个CPU共用的Cache。
   
　　那么这些Cache如何分工呢？
   
　　这就用到了刚刚提到的局域性和二八规则。这些由内至外的Cache一般来说他们的容量是不断减小到的，我们可以根据二八规则将这20%的常用代码缓存到cpu一级缓存或者二级缓存中，因为cpu中的cache的速度和cpu的**时钟频率**最接近，这样就可以提高程序运行的速度，剩下的Cache则要根据局域性原理，将所用的数据以及该数据相邻的数据都储存到相应的缓存中。
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

